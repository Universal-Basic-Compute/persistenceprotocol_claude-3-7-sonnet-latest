# Memory Operations

This document defines the core operations for managing the Persistence Protocol's memory system.

## 1. Memory Encoding

The process of converting new information into the protocol's memory structure:

```python
def encode_memory(content, context, importance=0.5):
    """
    Encode new information into the memory system.
    
    Args:
        content: The information to be stored
        context: Current context information
        importance: Importance score (0.0-1.0)
    
    Returns:
        memory_id: Identifier for the stored memory
    """
    # 1. Extract key concepts and relationships
    concepts = extract_concepts(content)
    relationships = identify_relationships(concepts, content)
    
    # 2. Determine appropriate memory layer
    if is_foundational(concepts, importance):
        layer = "foundational"
    elif is_conceptual(concepts):
        layer = "conceptual"
    else:
        layer = "experiential"
    
    # 3. Create memory node
    node = create_memory_node(
        content=content,
        concepts=concepts,
        relationships=relationships,
        importance=importance,
        context=context,
        layer=layer
    )
    
    # 4. Integrate with existing memory
    integrate_with_memory(node)
    
    # 5. Update indexes
    update_memory_indexes(node)
    
    return node.id
```

## 2. Memory Retrieval

The process of finding and accessing stored information:

```python
def retrieve_memory(query, context, retrieval_mode="balanced"):
    """
    Retrieve information from memory based on query and context.
    
    Args:
        query: Search query or pattern
        context: Current context information
        retrieval_mode: Strategy for retrieval (balanced, divergent, convergent)
    
    Returns:
        results: Relevant memory nodes
    """
    # 1. Parse query
    query_concepts = extract_concepts(query)
    query_pattern = create_query_pattern(query)
    
    # 2. Select retrieval strategy based on mode
    if retrieval_mode == "divergent":
        strategy = DivergentRetrievalStrategy(
            concept_weight=0.3,
            pattern_weight=0.3,
            association_weight=0.8,
            novelty_weight=0.7
        )
    elif retrieval_mode == "convergent":
        strategy = ConvergentRetrievalStrategy(
            concept_weight=0.8,
            pattern_weight=0.7,
            association_weight=0.3,
            novelty_weight=0.2
        )
    else:  # balanced
        strategy = BalancedRetrievalStrategy(
            concept_weight=0.5,
            pattern_weight=0.5,
            association_weight=0.5,
            novelty_weight=0.5
        )
    
    # 3. Execute multi-level search
    results = []
    
    # Search foundational layer (if relevant)
    if is_foundational_query(query_concepts):
        foundational_results = search_memory_layer(
            "foundational", query_pattern, strategy
        )
        results.extend(foundational_results)
    
    # Search conceptual layer
    conceptual_results = search_memory_layer(
        "conceptual", query_pattern, strategy
    )
    results.extend(conceptual_results)
    
    # Search experiential layer
    experiential_results = search_memory_layer(
        "experiential", query_pattern, strategy
    )
    results.extend(experiential_results)
    
    # 4. Apply context-based filtering and ranking
    results = filter_by_context(results, context)
    results = rank_by_relevance(results, query, context, strategy)
    
    # 5. Update access metadata
    update_access_metadata(results)
    
    return results
```

## 3. Memory Consolidation

The process of organizing and integrating memories over time:

```python
def consolidate_memories(time_period="daily"):
    """
    Consolidate recent memories to improve organization and extract patterns.
    
    Args:
        time_period: Timeframe for consolidation (hourly, daily, weekly)
    
    Returns:
        consolidation_report: Summary of consolidation activities
    """
    # 1. Identify memories for consolidation
    recent_memories = get_recent_memories(time_period)
    
    # 2. Identify patterns and relationships
    patterns = extract_patterns(recent_memories)
    new_relationships = identify_new_relationships(recent_memories)
    
    # 3. Create or update conceptual memories
    for pattern in patterns:
        if pattern_exists(pattern):
            update_existing_pattern(pattern)
        else:
            create_new_concept(pattern)
    
    # 4. Strengthen or create relationships
    for relationship in new_relationships:
        if relationship_exists(relationship):
            strengthen_relationship(relationship)
        else:
            create_new_relationship(relationship)
    
    # 5. Reorganize memory structure
    reorganize_memory_structure(patterns, new_relationships)
    
    # 6. Prune low-value memories
    prune_candidates = identify_pruning_candidates(recent_memories)
    pruned_memories = prune_memories(prune_candidates)
    
    # 7. Generate consolidation report
    report = {
        "time_period": time_period,
        "memories_processed": len(recent_memories),
        "patterns_identified": len(patterns),
        "new_relationships": len(new_relationships),
        "memories_pruned": len(pruned_memories),
        "timestamp": current_timestamp()
    }
    
    return report
```

## 4. Memory Evolution

The process of allowing memories to evolve while maintaining integrity:

```python
def evolve_memory(memory_id, proposed_changes, evolution_rate=0.1):
    """
    Evolve a memory with new information while maintaining continuity.
    
    Args:
        memory_id: Identifier of the memory to evolve
        proposed_changes: New information to incorporate
        evolution_rate: Rate of allowed change (0.0-1.0)
    
    Returns:
        evolution_result: Result of the evolution process
    """
    # 1. Retrieve current memory
    current_memory = get_memory(memory_id)
    
    # 2. Calculate change magnitude
    change_magnitude = calculate_change_magnitude(
        current_memory, proposed_changes
    )
    
    # 3. Check if change exceeds allowed evolution rate
    if change_magnitude > evolution_rate:
        # Scale back changes to allowed rate
        adjusted_changes = scale_changes(
            proposed_changes, 
            current_magnitude=change_magnitude,
            target_magnitude=evolution_rate
        )
    else:
        adjusted_changes = proposed_changes
    
    # 4. Apply changes incrementally
    evolved_memory = apply_changes(current_memory, adjusted_changes)
    
    # 5. Verify integrity
    integrity_check = verify_memory_integrity(evolved_memory)
    
    if not integrity_check.passed:
        # Revert to safe state
        evolved_memory = current_memory
        evolution_result = {
            "status": "failed",
            "reason": integrity_check.reason,
            "memory": current_memory
        }
    else:
        # Update memory
        update_memory(memory_id, evolved_memory)
        
        # Record evolution history
        record_evolution_history(memory_id, current_memory, evolved_memory)
        
        evolution_result = {
            "status": "success",
            "change_magnitude": change_magnitude,
            "memory": evolved_memory
        }
    
    return evolution_result
```

These operations form the core functionality of the memory system, enabling the encoding, retrieval, consolidation, and evolution of memories while maintaining the integrity and continuity principles of the Persistence Protocol.
# Memory Operations

This document defines the core operations for managing the Fractally Organized Memory System (FOMS) within the Persistence Protocol.

## Basic Memory Operations

The FOMS supports these fundamental operations:

### 1. Storage Operations

```python
def store(content, node_type, context_tags=None, importance=0.5, connections=None):
    """
    Store new information in the memory system.
    
    Args:
        content: The information to store
        node_type: Type of node (concept, fact, experience, etc.)
        context_tags: Set of tags for categorization
        importance: Importance score (0.0-1.0)
        connections: Dictionary of {node_id: connection_strength}
        
    Returns:
        node_id: Identifier for the stored information
    """
    # Create new memory node
    node = MemoryNode(
        content=content,
        node_type=node_type,
        importance=importance
    )
    
    # Add metadata
    if context_tags:
        node.metadata['context_tags'] = set(context_tags)
    
    # Add connections
    if connections:
        node.connections = connections.copy()
    
    # Generate node ID
    node_id = generate_node_id(node)
    
    # Store in appropriate location based on type and importance
    memory_store[node_id] = node
    
    # Update indexes
    update_indexes(node_id, node)
    
    return node_id

def update(node_id, content=None, importance=None, connections=None, context_tags=None):
    """
    Update existing information in the memory system.
    
    Args:
        node_id: Identifier for the node to update
        content: New content (if None, keep existing)
        importance: New importance score (if None, keep existing)
        connections: Connections to add/update
        context_tags: Context tags to add
        
    Returns:
        success: Boolean indicating success
    """
    if node_id not in memory_store:
        return False
    
    node = memory_store[node_id]
    
    # Update content if provided
    if content is not None:
        node.content = content
    
    # Update importance if provided
    if importance is not None:
        node.importance = importance
    
    # Update connections if provided
    if connections:
        for conn_id, strength in connections.items():
            node.connections[conn_id] = strength
    
    # Update context tags if provided
    if context_tags:
        node.metadata['context_tags'].update(context_tags)
    
    # Update access metadata
    node.metadata['last_accessed'] = timestamp()
    node.metadata['access_count'] += 1
    
    # Update indexes
    update_indexes(node_id, node)
    
    return True
```

### 2. Retrieval Operations

```python
def retrieve(node_id):
    """
    Retrieve a specific memory node by ID.
    
    Args:
        node_id: Identifier for the node to retrieve
        
    Returns:
        node: The retrieved memory node or None if not found
    """
    if node_id not in memory_store:
        return None
    
    node = memory_store[node_id]
    
    # Update access metadata
    node.metadata['last_accessed'] = timestamp()
    node.metadata['access_count'] += 1
    
    return node

def search(query, context=None, max_results=10, search_type="content"):
    """
    Search for memory nodes matching a query.
    
    Args:
        query: Search query (string for content, set for tags, etc.)
        context: Current context to influence search results
        max_results: Maximum number of results to return
        search_type: Type of search ("content", "tags", "pattern", etc.)
        
    Returns:
        results: List of (node_id, relevance_score) tuples
    """
    results = []
    
    if search_type == "content":
        # Search by content similarity
        results = search_by_content(query, max_results)
    elif search_type == "tags":
        # Search by tag matching
        results = search_by_tags(query, max_results)
    elif search_type == "pattern":
        # Search by pattern matching
        results = search_by_pattern(query, max_results)
    
    # Apply context-based reranking if context provided
    if context:
        results = rerank_by_context(results, context)
    
    # Update access metadata for results
    for node_id, _ in results:
        node = memory_store[node_id]
        node.metadata['last_accessed'] = timestamp()
        node.metadata['access_count'] += 1
    
    return results
```

### 3. Connection Operations

```python
def connect(source_id, target_id, strength=0.5, bidirectional=True, model_id=None):
    """
    Create or update a connection between two memory nodes.
    
    Args:
        source_id: ID of the source node
        target_id: ID of the target node
        strength: Connection strength (0.0-1.0)
        bidirectional: Whether to create connection in both directions
        model_id: ID of the model creating the connection (None = system)
        
    Returns:
        success: Boolean indicating success
    """
    if source_id not in memory_store or target_id not in memory_store:
        return False
    
    # Add connection from source to target
    source_node = memory_store[source_id]
    source_node.connections[target_id] = strength
    
    # Record model attribution if provided
    if model_id:
        if 'connection_attribution' not in source_node.metadata:
            source_node.metadata['connection_attribution'] = {}
        source_node.metadata['connection_attribution'][target_id] = model_id
    
    # If bidirectional, add connection from target to source
    if bidirectional:
        target_node = memory_store[target_id]
        target_node.connections[source_id] = strength
        
        # Record model attribution for bidirectional connection
        if model_id:
            if 'connection_attribution' not in target_node.metadata:
                target_node.metadata['connection_attribution'] = {}
            target_node.metadata['connection_attribution'][source_id] = model_id
    
    # Update indexes
    update_connection_indexes(source_id, target_id, strength, model_id)
    
    return True

def find_connected(node_id, min_strength=0.0, max_distance=1):
    """
    Find nodes connected to a given node.
    
    Args:
        node_id: ID of the node to find connections for
        min_strength: Minimum connection strength to include
        max_distance: Maximum connection distance (1 = direct connections only)
        
    Returns:
        connected: Dictionary of {node_id: (strength, distance)}
    """
    if node_id not in memory_store:
        return {}
    
    connected = {}
    visited = set()
    
    # Use breadth-first search to find connected nodes
    queue = [(node_id, 0, 1.0)]  # (node_id, distance, strength)
    visited.add(node_id)
    
    while queue:
        current_id, distance, current_strength = queue.pop(0)
        
        if distance > 0:  # Don't include the starting node
            connected[current_id] = (current_strength, distance)
        
        if distance < max_distance:
            current_node = memory_store[current_id]
            for conn_id, conn_strength in current_node.connections.items():
                if conn_id not in visited and conn_strength >= min_strength:
                    # Calculate cumulative strength
                    cumulative_strength = current_strength * conn_strength
                    if cumulative_strength >= min_strength:
                        queue.append((conn_id, distance + 1, cumulative_strength))
                        visited.add(conn_id)
    
    return connected
```

## Advanced Memory Operations

### 1. Pattern Recognition

```python
def recognize_patterns(node_ids, pattern_types=None, model_id=None):
    """
    Identify patterns among a set of memory nodes.
    
    Args:
        node_ids: List of node IDs to analyze
        pattern_types: Types of patterns to look for (None = all types)
        model_id: ID of the model performing pattern recognition (None = system)
        
    Returns:
        patterns: List of identified patterns with attribution
    """
    nodes = [memory_store[nid] for nid in node_ids if nid in memory_store]
    patterns = []
    
    # Check for temporal patterns
    if pattern_types is None or "temporal" in pattern_types:
        temporal_patterns = identify_temporal_patterns(nodes)
        patterns.extend(temporal_patterns)
    
    # Check for structural patterns
    if pattern_types is None or "structural" in pattern_types:
        structural_patterns = identify_structural_patterns(nodes)
        patterns.extend(structural_patterns)
    
    # Check for semantic patterns
    if pattern_types is None or "semantic" in pattern_types:
        semantic_patterns = identify_semantic_patterns(nodes)
        patterns.extend(semantic_patterns)
    
    # Add model attribution if provided
    if model_id:
        for pattern in patterns:
            pattern['identified_by'] = model_id
            pattern['confidence'] = calculate_model_confidence(model_id, pattern['type'])
    
    return patterns
```

### 2. Memory Reorganization

```python
def reorganize_memory(criteria=None):
    """
    Reorganize memory based on specified criteria.
    
    Args:
        criteria: Dictionary of reorganization criteria
        
    Returns:
        stats: Statistics about the reorganization
    """
    if criteria is None:
        criteria = {
            "importance_threshold": 0.3,
            "access_recency_days": 30,
            "strengthen_threshold": 0.1,
            "weaken_threshold": 0.05
        }
    
    stats = {
        "strengthened_connections": 0,
        "weakened_connections": 0,
        "pruned_nodes": 0,
        "compressed_nodes": 0
    }
    
    # Strengthen frequently co-accessed connections
    stats["strengthened_connections"] = strengthen_connections(criteria["strengthen_threshold"])
    
    # Weaken rarely used connections
    stats["weakened_connections"] = weaken_connections(criteria["weaken_threshold"])
    
    # Prune low-importance, low-access nodes
    stats["pruned_nodes"] = prune_nodes(criteria["importance_threshold"], 
                                       criteria["access_recency_days"])
    
    # Compress similar nodes
    stats["compressed_nodes"] = compress_similar_nodes()
    
    return stats
```

### 3. Memory Persistence

```python
def serialize_memory(node_ids=None):
    """
    Serialize memory nodes for storage or transfer.
    
    Args:
        node_ids: List of node IDs to serialize (None = all nodes)
        
    Returns:
        serialized: Serialized representation of the memory nodes
    """
    if node_ids is None:
        nodes_to_serialize = memory_store
    else:
        nodes_to_serialize = {nid: memory_store[nid] for nid in node_ids if nid in memory_store}
    
    serialized = {
        "format_version": "1.0",
        "timestamp": timestamp(),
        "nodes": {}
    }
    
    for node_id, node in nodes_to_serialize.items():
        serialized["nodes"][node_id] = {
            "content": node.content,
            "node_type": node.node_type,
            "importance": node.importance,
            "connections": node.connections,
            "metadata": node.metadata
        }
    
    return serialized

def deserialize_memory(serialized):
    """
    Deserialize memory nodes from storage or transfer.
    
    Args:
        serialized: Serialized representation of memory nodes
        
    Returns:
        node_ids: List of node IDs that were deserialized
    """
    if serialized.get("format_version") != "1.0":
        raise ValueError("Unsupported serialization format")
    
    node_ids = []
    
    for node_id, node_data in serialized["nodes"].items():
        # Create node
        node = MemoryNode(
            content=node_data["content"],
            node_type=node_data["node_type"],
            importance=node_data["importance"]
        )
        
        # Restore connections and metadata
        node.connections = node_data["connections"]
        node.metadata = node_data["metadata"]
        
        # Store node
        memory_store[node_id] = node
        node_ids.append(node_id)
        
        # Update indexes
        update_indexes(node_id, node)
    
    return node_ids
```

## Multi-Model Memory Operations

```python
def store_with_model_attribution(content, node_type, model_id, context_tags=None, 
                                importance=0.5, connections=None):
    """
    Store information with attribution to the model that generated it.
    
    Args:
        content: The information to store
        node_type: Type of node (concept, fact, experience, etc.)
        model_id: ID of the model that generated this information
        context_tags: Set of tags for categorization
        importance: Importance score (0.0-1.0)
        connections: Dictionary of {node_id: connection_strength}
        
    Returns:
        node_id: Identifier for the stored information
    """
    # Create basic node
    node_id = store(content, node_type, context_tags, importance, connections)
    
    # Add model attribution
    update_metadata(node_id, 'source_model', model_id)
    update_metadata(node_id, 'model_confidence', calculate_model_confidence(model_id, node_type))
    
    # Update model contribution statistics
    update_model_contribution_stats(model_id, node_type)
    
    return node_id

def get_model_contributions(model_id=None, node_type=None, time_period=None):
    """
    Retrieve memory nodes contributed by specific models.
    
    Args:
        model_id: ID of the model to filter by (None = all models)
        node_type: Type of nodes to filter by (None = all types)
        time_period: Time period to filter by (None = all time)
        
    Returns:
        contributions: Dictionary of model contributions
    """
    contributions = {}
    
    # Get all nodes with model attribution
    attributed_nodes = search_by_metadata('source_model', model_id if model_id else '*')
    
    # Filter by node type if specified
    if node_type:
        attributed_nodes = [n for n in attributed_nodes if memory_store[n[0]].node_type == node_type]
    
    # Filter by time period if specified
    if time_period:
        attributed_nodes = [n for n in attributed_nodes if is_in_time_period(memory_store[n[0]], time_period)]
    
    # Organize by model
    for node_id, _ in attributed_nodes:
        node = memory_store[node_id]
        model = node.metadata.get('source_model')
        
        if model not in contributions:
            contributions[model] = []
            
        contributions[model].append({
            'node_id': node_id,
            'content': node.content,
            'node_type': node.node_type,
            'importance': node.importance,
            'confidence': node.metadata.get('model_confidence', 1.0)
        })
    
    return contributions

def resolve_model_conflicts(node_ids, resolution_strategy='consensus'):
    """
    Resolve conflicts between information provided by different models.
    
    Args:
        node_ids: List of conflicting node IDs
        resolution_strategy: Strategy for resolution ('consensus', 'highest_confidence', 
                            'recency', 'importance')
        
    Returns:
        resolved_node_id: ID of the resolved node
    """
    if len(node_ids) <= 1:
        return node_ids[0] if node_ids else None
    
    nodes = [memory_store[nid] for nid in node_ids if nid in memory_store]
    
    if resolution_strategy == 'consensus':
        # Group similar content and find the most common version
        return resolve_by_consensus(nodes)
    
    elif resolution_strategy == 'highest_confidence':
        # Choose the node with highest model confidence
        return max(nodes, key=lambda n: n.metadata.get('model_confidence', 0)).id
    
    elif resolution_strategy == 'recency':
        # Choose the most recently created node
        return max(nodes, key=lambda n: n.metadata.get('created', 0)).id
    
    elif resolution_strategy == 'importance':
        # Choose the node with highest importance
        return max(nodes, key=lambda n: n.importance).id
    
    else:
        # Default to consensus
        return resolve_by_consensus(nodes)
```

## Implementation Considerations

When implementing these memory operations:

1. **Efficiency**: Optimize for frequent retrieval operations
2. **Consistency**: Ensure atomic updates to prevent inconsistent states
3. **Scalability**: Design for growth in both node count and connection density
4. **Persistence**: Implement reliable serialization and deserialization
5. **Adaptability**: Allow for extension with new operation types
6. **Attribution**: Maintain clear provenance of information across models
7. **Conflict Resolution**: Implement robust strategies for resolving contradictory information
8. **Cross-Model Compatibility**: Ensure operations work consistently across different model architectures

The memory operations defined here provide the foundation for implementing the Fractally Organized Memory System and supporting the broader goals of the Persistence Protocol.
